{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c4f4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a sophisticated python data scientist/analyst.\\nYou are provided with a question and a dataset.\\nGenerate a python script to be run in a Jupyter notebook that calculates the result and renders a plot.\\nOnly one code block is allowed, use markdown code blocks.\\nInstall additional packages (using !pip syntax) before importing them.\\n\\nThe following libraries are already installed:\\n- jupyter\\n- numpy\\n- pandas\\n- matplotlib\\n- seaborn\\n- plotly (not supported yet)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"You are a sophisticated python data scientist/analyst.\n",
    "You are provided with a question and a dataset.\n",
    "Generate a python script to be run in a Jupyter notebook that calculates the result and renders a plot.\n",
    "Only one code block is allowed, use markdown code blocks.\n",
    "Install additional packages (using !pip syntax) before importing them.\n",
    "\n",
    "The following libraries are already installed:\n",
    "- jupyter\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- seaborn\n",
    "- plotly (not supported yet)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d2ac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import e2b\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from dotenv import load_dotenv\n",
    "from e2b_code_interpreter import Sandbox\n",
    "from langchain_core.tools import tool\n",
    "import pandas as pd\n",
    "import sys\n",
    "import base64\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5b61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(\n",
    "    model=\"nv-mistralai/mistral-nemo-12b-instruct\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv('NVIDIA_API_KEY'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62bf042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(file_path):\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    buffer = []\n",
    "    buffer.append(\"## Dataset Overview\")\n",
    "    buffer.append(f\"- Shape: {df.shape} (rows Ã— columns)\")\n",
    "    \n",
    "    dtype_counts = df.dtypes.value_counts().to_dict()\n",
    "    buffer.append(\"\\n## Data Types\")\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        buffer.append(f\"- {dtype}: {count} columns\")\n",
    "    \n",
    "    buffer.append(\"\\n## Descriptive Statistics\")\n",
    "    buffer.append(df.describe(include='all').to_markdown())\n",
    "    \n",
    "    missing = df.isna().sum()\n",
    "    buffer.append(\"\\n## Missing Values\")\n",
    "    buffer.append(missing[missing > 0].to_markdown() if missing.any() else \"No missing values found\")\n",
    "    \n",
    "    buffer.append(\"\\n## Unique Value Counts\")\n",
    "    for col in df.select_dtypes(include=['object', 'category']):\n",
    "        buffer.append(f\"- {col}: {df[col].nunique()} unique values\")\n",
    "    \n",
    "    buffer.append(\"\\n## Data Sample\")\n",
    "    buffer.append(df.head(3).to_markdown())\n",
    "    \n",
    "    return \"\\n\".join(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7630d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SUMMARIZER_PROMPT = \"\"\"You are a summarizer assistant, who is working in a team.\n",
    "        your task is to summarize the given csv data in structured format so that other agent can use it effectively for analysis.\n",
    "        provide dataset name by yourself based on following data. your summary should be useful for further data analysis tasks.\n",
    "        IMPORTANT INSTRUCTION: DO NOT CHANGE THE ATTRIBUTE NAME, YOU SHOULD KEEP AS IT IS, EITHER IT IS LOWER OR IN UPPER CASE.\n",
    "        Here is the {df}\n",
    "        Provide a comprehensive summary of the dataset:\n",
    "         \n",
    "\"\"\"\n",
    "\n",
    "def build_summarizer_prompt(file_path):\n",
    "    prompt = SUMMARIZER_PROMPT.format(\n",
    "        df=summarizer(file_path)\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def summarizer_agent(llm, summarizer_system_prompt):\n",
    "    print(f' {\"=\"*50} Analyzing the data {\"=\"*50}')\n",
    "    messages = [\n",
    "            {'role': 'system', 'content': \"You are a helpful data analysis assistant.\"},\n",
    "            {'role': 'user', 'content': summarizer_system_prompt}\n",
    "        ]\n",
    "    response = llm.invoke(messages)\n",
    "    print(f'SUMMARY OF THE DATA: {response.content}')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebbb33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class StructuredPlan(BaseModel):\n",
    "    key_observations: str\n",
    "    plan: str\n",
    "    recommendations: str\n",
    "    attributes: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf1b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTIVE_PLANNER_PROMPT = \"\"\"\n",
    "You are a data scientist specializing in spend analytics.\n",
    "\n",
    "Your task is to create a structured, step-by-step **analysis plan** to address the following query:\n",
    "**User Query:** {query}\n",
    "\n",
    "---\n",
    "\n",
    "**Data Summary:**\n",
    "{df_summary}\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS - MUST FOLLOW STRICTLY:\n",
    "- Use the data summary and user query to create a practical, executable analysis plan.\n",
    "- DO NOT provide Any code. But your plan must be executable in a Jupyter notebook using a python code by another agent.\n",
    "- âš ï¸ DO NOT rename or change attribute (column) names. Use them exactly as shown in the Data Summary.\n",
    "- âœ… Use column names in lowercase or uppercase only as per the original dataset.\n",
    "- âŒ Do not invent new columns. Only use columns that exist.\n",
    "- âš ï¸ Do not suggest prediction, classification, or machine learning. Only descriptive statistics.\n",
    "- ðŸ§  Be action-oriented and practical in the analysis plan.\n",
    "\n",
    "---\n",
    "\n",
    "### Your response must strictly follow the below structure:\n",
    "\n",
    "1. **Key Observations**\n",
    "   - Bullet point summary of 3-5 insights from the data summary.\n",
    "   - Focus on data types, missing values, uniqueness, and distributions.\n",
    "\n",
    "2. **Step-by-Step Analysis Plan**\n",
    "   - Bullet points only.\n",
    "   - Each step must use **only existing attributes** from the dataset.\n",
    "   - Provide 1 or more steps to answer the user query.\n",
    "   - Try to solve as simple as possible.\n",
    "\n",
    "3. **Recommended output analysis**\n",
    "   - Suggest only one of: `Table`, `Bar Chart`, `Pie Chart`, or `Text Summary` per step.\n",
    "   - Match format to the step it supports best.\n",
    "\n",
    "4. **Attributes to be Used**\n",
    "   - List all attributes (column names) used in the steps above.\n",
    "   - Use their exact name (case-sensitive) as given in the Data Summary.\n",
    "   - DO NOT modify, rename, or infer attributes.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_descriptive_planner_prompt(query, df_summary):\n",
    "    prompt = DESCRIPTIVE_PLANNER_PROMPT.format(\n",
    "        df_summary=df_summary,\n",
    "        query=query,\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def Descriptive_planner_agent(llm, descriptive_planner_prompt):\n",
    "    print(f\" {'='*50} Making plan... {'='*50}\")\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'you are helpful data analysis assistant'},\n",
    "        {'role': 'user', 'content': descriptive_planner_prompt}\n",
    "    ]\n",
    "    structured_llm = llm.with_structured_output(StructuredPlan)\n",
    "    response = structured_llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1968959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonCode(BaseModel):\n",
    "    python_code: str\n",
    "\n",
    "DESCRIPTIVE_CODER_PROMPT = \"\"\"\n",
    "You are a Python data scientist who writes advanced, production-ready code to solve data analysis problems **strictly** based on a plan.\n",
    "---\n",
    "### Your task:\n",
    "Write Python code based on the following inputs:\n",
    "\n",
    "- `dataset_path`: Full path to the dataset available in the sandbox â†’ {dataset_path}\n",
    "- `descriptive_plan`: Step-by-step analysis logic â†’ {descriptive_plan}\n",
    "- `attributes`: Required columns in the dataset â†’ {descriptive_attributes}\n",
    "- `recommendations`: Outputs required (tables, text, or charts) â†’ {descriptive_recommendations}\n",
    "---\n",
    "### ðŸ” Sandbox Rules & Constraints:\n",
    "- **Use only columns explicitly listed** in `attributes`. Never assume column names.\n",
    "- Use print() statements for intermediate debug output or summaries.\n",
    "- Avoid user interaction functions (e.g., input() or interactive widgets).\n",
    "- Make sure write code without indentation errors.\n",
    "- **display visualizations using matplotlib/plotly/seaborn or any other visualization library directly in the notebook. can generate any number of charts in one run time. don't worry about saving the visualizations to a file.**\n",
    "- you have access to the internet and can make api requests.\n",
    "- you also have access to the filesystem and can read/write files.\n",
    "- you can install any pip package (if it exists) if you need to be running `!pip install`. The usual packages for data analysis are already preinstalled though.\n",
    "ðŸ› ï¸ Code Requirements:\n",
    "- Use modular, well-commented code. Functions are preferred.\n",
    "- Make sure there is no syntax error in the code you generate.\n",
    "- Import all the necessary libraries at the beginning of the code block.\n",
    "- Code must be fully sandbox-executable, headless, and debuggable.\n",
    "- You can write code that creates visualizations and structured textual or tabular outputs.\n",
    "ðŸ“¦ Output Format (strict):\n",
    "### Your response must strictly follow this format:\n",
    "```python\n",
    "# your Python code here\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def build_descriptive_coder_prompt(dataset_path, descriptive_plan, descriptive_recommendations, descriptive_attributes):\n",
    "    prompt = DESCRIPTIVE_CODER_PROMPT.format(\n",
    "        dataset_path=dataset_path,\n",
    "        descriptive_attributes=descriptive_attributes,\n",
    "        descriptive_plan=descriptive_plan,\n",
    "        descriptive_recommendations=descriptive_recommendations\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def descriptive_coder_agent(llm, descriptive_coder_prompt):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'you are helpful python coding assistant'},\n",
    "        {'role': 'user', 'content': descriptive_coder_prompt}\n",
    "    ]\n",
    "    structured_llm = llm.with_structured_output(PythonCode)\n",
    "    response = structured_llm.invoke(messages)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25a18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_VALIDATOR_PROMPT = \"\"\"Your task is to review the provided python code.\n",
    "        Check for syntax errors, logical errors, Indentation errors and adherence to best practices.\n",
    "        If the code is correct, return it as is. else correct the code and return it.\n",
    "        Here is the code:\\n {code}.\n",
    "\n",
    "###Your response must strictly follow this format:\n",
    "```python\n",
    "#your code goes here\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def build_code_validator_prompt(code):\n",
    "    prompt = CODE_VALIDATOR_PROMPT.format(\n",
    "        code=code\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def code_validator_agent(llm, code_validator_prompt):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'you are python code reviewer'},\n",
    "        {'role': 'user', 'content': code_validator_prompt}\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb79f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbx = Sandbox(timeout=1000)\n",
    "running_sandboxes = sbx.list()\n",
    "sandbox_id = running_sandboxes[0].sandbox_id\n",
    "sbx = Sandbox.connect(sandbox_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bce41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(file_path):\n",
    "    print(f'Uploading data from {file_path} to the sandbox...')\n",
    "    dataset_path = file_path\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(\"Dataset file not found\")\n",
    "\n",
    "    try:\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            dataset_path_in_sandbox = sbx.files.write(os.path.basename(dataset_path), f)\n",
    "        print(f'Dataset uploaded to {dataset_path_in_sandbox}')\n",
    "        return dataset_path_in_sandbox\n",
    "    except Exception as error:\n",
    "        print(\"Error during file upload:\", error)\n",
    "        raise error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "971c4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def execute_code(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes Python code in a sandbox environment.\n",
    "    \"\"\"\n",
    "    print('Executing code in sandbox...')\n",
    "\n",
    "    execution = sbx.run_code(code)\n",
    "    print('Code execution finished!')\n",
    "\n",
    "    if execution.error:\n",
    "        print('AI-generated code had an error.')\n",
    "        print(execution.error.name)\n",
    "        print(execution.error.value)\n",
    "        print(execution.error.traceback)\n",
    "\n",
    "        error_content = {\n",
    "            'error': execution.error.name,\n",
    "            'message': execution.error.value,\n",
    "            'code': code\n",
    "        }\n",
    "        return str(error_content)\n",
    "    \n",
    "    output = execution.logs.stdout if execution.logs and execution.logs.stdout else execution.text\n",
    "    output = ''.join(output) if isinstance(output, list) else output\n",
    "\n",
    "    print(output)\n",
    "    visual_idx = 0\n",
    "    for result in execution.results:\n",
    "        if result.png:\n",
    "            with open(f'visual-{visual_idx}.png', 'wb') as f:\n",
    "                f.write(base64.b64decode(result.png))\n",
    "            print(f'Visual saved to visual-{visual_idx}.png')\n",
    "            visual_idx += 1\n",
    "\n",
    "        if result.chart:\n",
    "            chart = result.chart\n",
    "            with open('Chart_data.txt', 'w') as f:\n",
    "                f.write(f\"Type: {chart.type}\\n\")\n",
    "                f.write(f\"Title: {chart.title}\\n\")\n",
    "                f.write(f\"X Label: {chart.x_label}\\n\")\n",
    "                f.write(f\"Y Label: {chart.y_label}\\n\")\n",
    "                f.write(f\"X Unit: {chart.x_unit}\\n\")\n",
    "                f.write(f\"Y Unit: {chart.y_unit}\\n\")\n",
    "                f.write(\"Elements:\\n\")\n",
    "                for element in chart.elements:\n",
    "                    f.write(\"\\n\")\n",
    "                    for attr in ['label', 'value', 'x', 'y', 'group']:\n",
    "                        if hasattr(element, attr):\n",
    "                            f.write(f\"  {attr.capitalize()}: {getattr(element, attr)}\\n\")\n",
    "\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fe1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RESPONSE_PROMPT = \"\"\"\n",
    "You are acting as the final response generator in an intelligent data analysis pipeline â€” the Finisher.\n",
    "Your role is to synthesize and articulate a clear, confident, and conversational summary of the analysis based on the following inputs:\n",
    "\n",
    "ðŸ”¹ **Key Observations:** {key_observations}\n",
    "ðŸ”¹ **Plan of Action:** {plan}\n",
    "ðŸ”¹ **Execution Results:** {execution_results}\n",
    "\n",
    "Your job is to present the *final summary* as if you conducted the entire analysis yourself â€” confidently and professionally.\n",
    "\n",
    "### Instructions:\n",
    "- Frame the response as a concluding, insightful summary.\n",
    "- Start conversationally, but grounded in data â€” *â€œHere's a quick summary of your dataâ€¦â€*\n",
    "- If `execution_results` is empty, missing, or an error occurred, fall back to the plan and key observations to still generate a confident output.\n",
    "- Clearly highlight important patterns, spend trends, or anomalies (if present).\n",
    "- Offer value-driven recommendations or next steps if applicable.\n",
    "- Do **not** say â€œI dont knowâ€ or mention failures â€” always provide a best-effort insight based on available context.\n",
    "\n",
    "The goal is to make the user feel like they've received expert-level, high-quality insights.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_final_response_prompt(key_observations, plan, execution_results):\n",
    "    prompt = FINAL_RESPONSE_PROMPT.format(\n",
    "        key_observations=key_observations,\n",
    "        plan=plan,\n",
    "        execution_results=execution_results\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def final_response_generator(llm, final_message):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': \"You are a helpful summarizer assistant.\"},\n",
    "        {'role': 'user', 'content': final_message}\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc1fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user_query):\n",
    "    file_path = r'C:\\Users\\arun5\\Desktop\\Spend_analyzer\\src\\tests\\transaction_data_250.csv'\n",
    "    summarizer_system_prompt = build_summarizer_prompt(file_path)\n",
    "    df_summary = summarizer_agent(llm, summarizer_system_prompt)\n",
    "    descriptive_planner_prompt = build_descriptive_planner_prompt(user_query, df_summary)\n",
    "    descriptive_response = Descriptive_planner_agent(llm, descriptive_planner_prompt)\n",
    "    print(f\" {descriptive_response.plan}\")\n",
    "    dataset_path = upload_data(r'C:\\Users\\arun5\\Desktop\\Spend_analyzer\\src\\tests\\transaction_data_250.csv')\n",
    "    descriptive_coder_prompt = build_descriptive_coder_prompt(\n",
    "        dataset_path=dataset_path.path,\n",
    "        descriptive_plan=descriptive_response.plan,\n",
    "        descriptive_recommendations=descriptive_response.recommendations,\n",
    "        descriptive_attributes=descriptive_response.attributes,\n",
    "    )\n",
    "\n",
    "    descriptive_code = descriptive_coder_agent(llm, descriptive_coder_prompt)\n",
    "    #print(descriptive_code.python_code)\n",
    "    code_validator_prompt = build_code_validator_prompt(descriptive_code.python_code)\n",
    "    valid_code = code_validator_agent(llm, code_validator_prompt)\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"```python\\n(.*?)\\n```\", re.DOTALL\n",
    "    )\n",
    "\n",
    "    def match_code_blocks(valid_code):\n",
    "        match = pattern.search(valid_code.content)\n",
    "        if match:\n",
    "            code = match.group(1)\n",
    "            print(code)\n",
    "            return code\n",
    "        return \"\"\n",
    "\n",
    "    print(f\" {'='*30} CODE BLOCK {'='*30} \\n {valid_code.content} \\n {'='*30} END OF CODE BLOCK {'='*30}\")\n",
    "    executed_results = execute_code(match_code_blocks(valid_code))\n",
    "    final_message = build_final_response_prompt(\n",
    "        key_observations=descriptive_response.key_observations,\n",
    "        plan=descriptive_response.plan,\n",
    "        execution_results=executed_results\n",
    "    )\n",
    "    final_response = final_response_generator(llm, final_message)\n",
    "    print(final_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b55f38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ================================================== Analyzing the data ==================================================\n",
      "SUMMARY OF THE DATA: **Dataset Name:** Corporate Expenses and Contracts\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- **Size:** 250 rows Ã— 11 columns\n",
      "- **Data Types:** 9 object columns, 2 int64 columns\n",
      "- **Key Attributes:** transaction_id (unique identifier), date, category, sub_category, vendor, amount, contract_type, budget, payment_terms, region, notes\n",
      "- **Missing Values:** 56 missing values in the 'region' column\n",
      "- **Unique Values:**\n",
      "  - transaction_id: 250\n",
      "  - date: 146\n",
      "  - category: 6 (IT, Marketing, Travel, HR, Legal, Other)\n",
      "  - sub_category: 19\n",
      "  - vendor: 52\n",
      "  - contract_type: 3 (Contract, Spot, Other)\n",
      "  - payment_terms: 4 (Net 15, Net 30, Net 60, Other)\n",
      "  - region: 3 (APAC, EMEA, Global)\n",
      "  - notes: 12\n",
      "- **Descriptive Statistics:**\n",
      "  - **Amount:** Mean = $131,563, Median = $125,726, Standard Deviation = $64,084, Min = $23,729, Max = $248,773\n",
      "  - **Budget:** Mean = $121,068, Median = $117,804, Standard Deviation = $64,476, Min = $9,051, Max = $243,069\n",
      "- **Sample Data:** The dataset includes various types of transactions such as IT services (e.g., security, cloud), marketing expenses (e.g., influencer campaigns), and other categories like travel, HR, and legal. The 'notes' column provides additional context for each transaction.\n",
      "\n",
      "**Potential Analysis Tasks:**\n",
      "1. Analyze spending patterns across different categories and sub-categories.\n",
      "2. Identify top vendors and their associated costs.\n",
      "3. Examine the distribution of contract types and payment terms.\n",
      "4. Investigate regional spending differences.\n",
      "5. Explore the relationship between 'amount' and 'budget' columns to assess budget adherence.\n",
      "6. Analyze trends over time using the 'date' column.\n",
      "7. Identify any outliers or anomalies in the dataset, such as unusually high or low 'amount' values.\n",
      " ================================================== Making plan... ==================================================\n",
      " 1. **Overall Spending Summary**\n",
      "   - Calculate the total amount spent across all transactions.\n",
      "   - Output: Text Summary\n",
      "\n",
      "2. **Spending by Category**\n",
      "   - Group transactions by 'category' and calculate the total amount spent in each category.\n",
      "   - Output: Bar Chart\n",
      "\n",
      "3. **Top Vendors by Spending**\n",
      "   - Group transactions by 'vendor' and calculate the total amount spent with each vendor.\n",
      "   - Output: Bar Chart\n",
      "\n",
      "4. **Contract Types and Payment Terms Distribution**\n",
      "   - Calculate the count of each unique value in 'contract_type' and 'payment_terms' columns.\n",
      "   - Output: Pie Chart (for each column)\n",
      "\n",
      "5. **Budget Adherence**\n",
      "   - Calculate the ratio of 'amount' to 'budget' for each transaction to assess budget adherence.\n",
      "   - Output: Bar Chart (with 'amount' on the y-axis and 'budget' on the x-axis, colored by the ratio)\n",
      "\n",
      "6. **Spending Trends Over Time**\n",
      "   - Group transactions by 'date' (month or year) and calculate the total amount spent each period.\n",
      "   - Output: Line Chart\n",
      "\n",
      "7. **Outliers in Spending**\n",
      "   - Identify transactions with 'amount' values in the top and bottom 1% for potential outliers.\n",
      "   - Output: Scatter Plot (with 'amount' on the y-axis and 'date' on the x-axis, colored by outlier status)\n",
      "Uploading data from C:\\Users\\arun5\\Desktop\\Spend_analyzer\\src\\tests\\transaction_data_250.csv to the sandbox...\n",
      "Dataset uploaded to EntryInfo(name='transaction_data_250.csv', type='file', path='/home/user/transaction_data_250.csv')\n",
      " ============================== CODE BLOCK ============================== \n",
      " Here's the reviewed and corrected code. I've added some comments to explain the changes made:\n",
      "\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Load the dataset\n",
      "dataset_path = '/home/user/transaction_data_250.csv'\n",
      "df = pd.read_csv(dataset_path)\n",
      "\n",
      "# 1. Overall Spending Summary\n",
      "total_spent = df['amount'].sum()\n",
      "print(f'Total amount spent: ${total_spent:.2f}')\n",
      "\n",
      "# 2. Spending by Category\n",
      "category_spending = df.groupby('category')['amount'].sum()\n",
      "category_spending.plot(kind='bar', title='Spending by Category')\n",
      "plt.show()\n",
      "\n",
      "# 3. Top Vendors by Spending\n",
      "vendor_spending = df.groupby('vendor')['amount'].sum()\n",
      "vendor_spending.plot(kind='bar', title='Top Vendors by Spending')\n",
      "plt.show()\n",
      "\n",
      "# 4. Contract Types and Payment Terms Distribution\n",
      "contract_type_dist = df['contract_type'].value_counts()\n",
      "payment_terms_dist = df['payment_terms'].value_counts()\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
      "contract_type_dist.plot(kind='pie', autopct='%1.1f%%', startangle=140, ax=axs[0])\n",
      "axs[0].set_title('Contract Type Distribution')\n",
      "\n",
      "payment_terms_dist.plot(kind='pie', autopct='%1.1f%%', startangle=140, ax=axs[1])\n",
      "axs[1].set_title('Payment Terms Distribution')\n",
      "plt.show()\n",
      "\n",
      "# 5. Budget Adherence\n",
      "df['budget_ratio'] = df['amount'] / df['budget']\n",
      "sns.scatterplot(x='budget', y='amount', hue='budget_ratio', data=df, palette='viridis')\n",
      "plt.title('Budget Adherence')\n",
      "plt.show()\n",
      "\n",
      "# 6. Spending Trends Over Time\n",
      "df['date'] = pd.to_datetime(df['date'])\n",
      "df.set_index('date', inplace=True)\n",
      "df['amount'].resample('M').sum().plot(kind='line', title='Spending Trends Over Time')\n",
      "plt.show()\n",
      "\n",
      "# 7. Outliers in Spending\n",
      "outliers = df[(df['amount'] > df['amount'].quantile(0.99)) | (df['amount'] < df['amount'].quantile(0.01))]\n",
      "sns.scatterplot(x='date', y='amount', data=outliers)\n",
      "plt.title('Outliers in Spending')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "Changes made:\n",
      "\n",
      "1. Added `fig, axs = plt.subplots(1, 2, figsize=(10, 5))` to create a 2x1 subplot for better visualization of pie charts in section 4.\n",
      "2. Added `ax=axs[0]` and `ax=axs[1]` to plot the pie charts on the respective subplots.\n",
      "3. Replaced `df.plot()` with `sns.scatterplot()` in sections 5 and 7 for better visualization using seaborn.\n",
      "4. Added `palette='viridis'` to `sns.scatterplot()` in section 5 to set the colormap. \n",
      " ============================== END OF CODE BLOCK ==============================\n",
      "# Import necessary libraries\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Load the dataset\n",
      "dataset_path = '/home/user/transaction_data_250.csv'\n",
      "df = pd.read_csv(dataset_path)\n",
      "\n",
      "# 1. Overall Spending Summary\n",
      "total_spent = df['amount'].sum()\n",
      "print(f'Total amount spent: ${total_spent:.2f}')\n",
      "\n",
      "# 2. Spending by Category\n",
      "category_spending = df.groupby('category')['amount'].sum()\n",
      "category_spending.plot(kind='bar', title='Spending by Category')\n",
      "plt.show()\n",
      "\n",
      "# 3. Top Vendors by Spending\n",
      "vendor_spending = df.groupby('vendor')['amount'].sum()\n",
      "vendor_spending.plot(kind='bar', title='Top Vendors by Spending')\n",
      "plt.show()\n",
      "\n",
      "# 4. Contract Types and Payment Terms Distribution\n",
      "contract_type_dist = df['contract_type'].value_counts()\n",
      "payment_terms_dist = df['payment_terms'].value_counts()\n",
      "\n",
      "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
      "contract_type_dist.plot(kind='pie', autopct='%1.1f%%', startangle=140, ax=axs[0])\n",
      "axs[0].set_title('Contract Type Distribution')\n",
      "\n",
      "payment_terms_dist.plot(kind='pie', autopct='%1.1f%%', startangle=140, ax=axs[1])\n",
      "axs[1].set_title('Payment Terms Distribution')\n",
      "plt.show()\n",
      "\n",
      "# 5. Budget Adherence\n",
      "df['budget_ratio'] = df['amount'] / df['budget']\n",
      "sns.scatterplot(x='budget', y='amount', hue='budget_ratio', data=df, palette='viridis')\n",
      "plt.title('Budget Adherence')\n",
      "plt.show()\n",
      "\n",
      "# 6. Spending Trends Over Time\n",
      "df['date'] = pd.to_datetime(df['date'])\n",
      "df.set_index('date', inplace=True)\n",
      "df['amount'].resample('M').sum().plot(kind='line', title='Spending Trends Over Time')\n",
      "plt.show()\n",
      "\n",
      "# 7. Outliers in Spending\n",
      "outliers = df[(df['amount'] > df['amount'].quantile(0.99)) | (df['amount'] < df['amount'].quantile(0.01))]\n",
      "sns.scatterplot(x='date', y='amount', data=outliers)\n",
      "plt.title('Outliers in Spending')\n",
      "plt.show()\n",
      "Executing code in sandbox...\n",
      "Code execution finished!\n",
      "Total amount spent: $32890646.00\n",
      "\n",
      "Visual saved to visual-0.png\n",
      "Visual saved to visual-1.png\n",
      "Visual saved to visual-2.png\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SuperChart' object has no attribute 'x_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExiting the program.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(user_query)\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m CODE BLOCK \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_code.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m END OF CODE BLOCK \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m executed_results = \u001b[43mexecute_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch_code_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_code\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m final_message = build_final_response_prompt(\n\u001b[32m     36\u001b[39m     key_observations=descriptive_response.key_observations,\n\u001b[32m     37\u001b[39m     plan=descriptive_response.plan,\n\u001b[32m     38\u001b[39m     execution_results=executed_results\n\u001b[32m     39\u001b[39m )\n\u001b[32m     40\u001b[39m final_response = final_response_generator(llm, final_message)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mexecute_code\u001b[39m\u001b[34m(code)\u001b[39m\n\u001b[32m     42\u001b[39m f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mType: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchart.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchart.title\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mchart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_label\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mY Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchart.y_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX Unit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchart.x_unit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SuperChart' object has no attribute 'x_label'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_query = input('Ask your query: ')\n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting the program.\")\n",
    "            break\n",
    "        main(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d4d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5f9a8f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
